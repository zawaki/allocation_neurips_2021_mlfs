# Resource Allocation in Disaggregated Data Centre Systems with Reinforcement Learning (Code)

This repository contains the learning and simulation models used to train and test the deep reinforcement learning based network-aware resource allocator presented in the paper (https://arxiv.org/abs/2106.02412) of the same title, which was presented in the Machine Learning for Systems Conference at NeurIPS 2021.

This README will detail how the code is run in order to produce the same training and testing as in the paper on a given topology, where a small and large topology used in the presented work are also provided.

## Installing the relevant modules


In the same directory as this document, simply run:

```bash
./install_modules.sh
```


## Training

In the ```./scripts``` directory, simply run:

```bash
python train.py --save_dir=<PATH TO INTENDED SAVE DIRECTORY>
```

If no ```save_dir``` argument is included, then it will default to the same directory that contains the ```train.py``` script.

This script will save a checkpoint of the agent every training iteration, as per the standard ```Rllib``` training procedure. These checkpoints can then be loaded (again, as per the ```Rllib``` API) and used for testing. 

## Testing

### Run a test

Tests can be run with either:

1. A pre-trained reinforcement learning agent.
2. The heuristics used in the paper (Tetris, NALB, NULB, random).

Similarly to training, testing is run by typing the command (in the ```./scripts``` directory):

```bash
python test.py --test_baselines=<str in {'yes','no'}> --topology=<str in {'small','large'}> --agent_checkpoint=<PATH TO TRAINING CHECKPOINT OF AGENT TO BE TESTED> --save_dir=<PATH TO THE INTENDED SAVE DIRECTORY> --iterations=<NUMBER OF TEST ITERATIONS TO DO PER METHOD>
```

Since baselines only need to be tested once (they are static policies) the ```--test_baselines='yes'``` argument will run each of the 4 baselines on the given topology. This should generally only be done once per topology since the baselines are static. This argument will also default to 'no'.

### Saving the test data

The test script will (if no such existing directory is found) create a separate directory for each method in the specified ```save_dir``` location, where this directory will be named either 'agent', 'tetris', 'nalb', 'nulb', or 'random' corresponding to which method is being tested. Otherwise, the results in this directory will be overwritten!

During a training run, the rollout will generate the following data (where the ```#``` in the filename will correspond to the which test iteration that data was generated by):

#### ac_bw_features_#.csv
* each row indicates the utilisation of the ports resoruce on the aggregate-core (tier-3) links for each link of this type for each event iteration in the test episode.

#### ra_bw_features_#.csv
* each row indicates the utilisation of the ports resoruce on the rack-aggregate (tier-2) links for each link of this type for each event iteration in the test episode.

#### sr_bw_features_#.csv
* each row indicates the utilisation of the ports resoruce on the server-rack (tier-1) links for each link of this type for each event iteration in the test episode.

#### acceptance_#.csv
* each row contains a single value of the acceptance ratio (one row per event iteration in test episode).

#### cpu_features_#.csv
* each row indicates the utilisation of the CPU resoruce on each node in the network.

#### mem_features_#.csv
* each row indicates the utilisation of the memory resoruce on each node in the network.

#### util_#.csv
* each row indicates the values for the following named metrics (listed from left column to right) at each iteration in the test episode.
    * compute (CPU utilisation)
    * memory (memory utilisation)
    * bandwidth (network-wide port utilisation)
    * compute_fragmentation (N/A)
    * memory_fragmentation (N/A)

####  success_#.csv & failure_#.csv
* each row shows the following information (listed from left column to right) about requests which were successfully and unsuccessfully allocation respectively:
    * compute (requested amount)
    * memory (requested amount)
    * bandwidth (N/A)
    * holding_time (requested amount)
    * node_link_ratio (ratio of number of allocated nodes vs number of allocated links - only for successfull requests)
    * num_nodes (number of nodes that were allocated to this request - only for successfull requests)

#### actions_#.csv
* each row shows the following information about the actions taken during the test episode for both successful and unsuccessful requests (listed from left column to right):
    * action (integer ID of the server - corresponding to the DGL topology used during runtime - chosen)
    * request_id (integer ID of the request for which this action was taken.)

